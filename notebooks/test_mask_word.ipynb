{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from underthesea import word_tokenize\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "from fairseq.data.encoders.fastbpe import fastBPE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _load_vectors(fname, num_words=10000):\n",
    "    \"\"\"Load fasttext vector\n",
    "\n",
    "    Args:\n",
    "        fname (str): file path\n",
    "        num_words (int, optional): Number of words to be loaded. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary of word\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    fin.readline().split()\n",
    "    data = {}\n",
    "    for i, line in enumerate(tqdm(fin)):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array([float(val) for val in tokens[1:]])\n",
    "        data[tokens[0]] /= np.linalg.norm(data[tokens[0]])\n",
    "        if i > num_words:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [00:01, 6386.85it/s]\n"
     ]
    }
   ],
   "source": [
    "FASTTEXT_PATH = \"../augment_model/cc.vi.300.vec\"\n",
    "fasttext_data = _load_vectors(FASTTEXT_PATH)\n",
    "\n",
    "class BPE():\n",
    "    bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 17:04:38 | INFO | fairseq.file_utils | loading archive file ../PhoBERT_base_fairseq\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Model file not found: ../PhoBERT_base_fairseq/../model.pt",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb Cell 4'\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001B[0m phobert \u001B[39m=\u001B[39m RobertaModel\u001B[39m.\u001B[39;49mfrom_pretrained(\u001B[39m'\u001B[39;49m\u001B[39m../PhoBERT_base_fairseq\u001B[39;49m\u001B[39m'\u001B[39;49m, checkpoint_file\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39m../model.pt\u001B[39;49m\u001B[39m'\u001B[39;49m)\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001B[0m args \u001B[39m=\u001B[39m BPE()\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001B[0m phobert\u001B[39m.\u001B[39mbpe \u001B[39m=\u001B[39m fastBPE(args)\n",
      "File \u001B[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/models/roberta/model.py:244\u001B[0m, in \u001B[0;36mRobertaModel.from_pretrained\u001B[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, bpe, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[39m@classmethod\u001B[39m\n\u001B[1;32m    234\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mfrom_pretrained\u001B[39m(\n\u001B[1;32m    235\u001B[0m     \u001B[39mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    240\u001B[0m     \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs\n\u001B[1;32m    241\u001B[0m ):\n\u001B[1;32m    242\u001B[0m     \u001B[39mfrom\u001B[39;00m \u001B[39mfairseq\u001B[39;00m \u001B[39mimport\u001B[39;00m hub_utils\n\u001B[0;32m--> 244\u001B[0m     x \u001B[39m=\u001B[39m hub_utils\u001B[39m.\u001B[39;49mfrom_pretrained(\n\u001B[1;32m    245\u001B[0m         model_name_or_path,\n\u001B[1;32m    246\u001B[0m         checkpoint_file,\n\u001B[1;32m    247\u001B[0m         data_name_or_path,\n\u001B[1;32m    248\u001B[0m         archive_map\u001B[39m=\u001B[39;49m\u001B[39mcls\u001B[39;49m\u001B[39m.\u001B[39;49mhub_models(),\n\u001B[1;32m    249\u001B[0m         bpe\u001B[39m=\u001B[39;49mbpe,\n\u001B[1;32m    250\u001B[0m         load_checkpoint_heads\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m,\n\u001B[1;32m    251\u001B[0m         \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs,\n\u001B[1;32m    252\u001B[0m     )\n\u001B[1;32m    253\u001B[0m     \u001B[39mreturn\u001B[39;00m RobertaHubInterface(x[\u001B[39m\"\u001B[39m\u001B[39margs\u001B[39m\u001B[39m\"\u001B[39m], x[\u001B[39m\"\u001B[39m\u001B[39mtask\u001B[39m\u001B[39m\"\u001B[39m], x[\u001B[39m\"\u001B[39m\u001B[39mmodels\u001B[39m\u001B[39m\"\u001B[39m][\u001B[39m0\u001B[39m])\n",
      "File \u001B[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/hub_utils.py:70\u001B[0m, in \u001B[0;36mfrom_pretrained\u001B[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39muser_dir\u001B[39m\u001B[39m\"\u001B[39m \u001B[39min\u001B[39;00m kwargs:\n\u001B[1;32m     68\u001B[0m     utils\u001B[39m.\u001B[39mimport_user_module(argparse\u001B[39m.\u001B[39mNamespace(user_dir\u001B[39m=\u001B[39mkwargs[\u001B[39m\"\u001B[39m\u001B[39muser_dir\u001B[39m\u001B[39m\"\u001B[39m]))\n\u001B[0;32m---> 70\u001B[0m models, args, task \u001B[39m=\u001B[39m checkpoint_utils\u001B[39m.\u001B[39;49mload_model_ensemble_and_task(\n\u001B[1;32m     71\u001B[0m     [os\u001B[39m.\u001B[39;49mpath\u001B[39m.\u001B[39;49mjoin(model_path, cpt) \u001B[39mfor\u001B[39;49;00m cpt \u001B[39min\u001B[39;49;00m checkpoint_file\u001B[39m.\u001B[39;49msplit(os\u001B[39m.\u001B[39;49mpathsep)],\n\u001B[1;32m     72\u001B[0m     arg_overrides\u001B[39m=\u001B[39;49mkwargs,\n\u001B[1;32m     73\u001B[0m )\n\u001B[1;32m     75\u001B[0m \u001B[39mreturn\u001B[39;00m {\n\u001B[1;32m     76\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39margs\u001B[39m\u001B[39m\"\u001B[39m: args,\n\u001B[1;32m     77\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mtask\u001B[39m\u001B[39m\"\u001B[39m: task,\n\u001B[1;32m     78\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mmodels\u001B[39m\u001B[39m\"\u001B[39m: models,\n\u001B[1;32m     79\u001B[0m }\n",
      "File \u001B[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/checkpoint_utils.py:278\u001B[0m, in \u001B[0;36mload_model_ensemble_and_task\u001B[0;34m(filenames, arg_overrides, task, strict, suffix, num_shards)\u001B[0m\n\u001B[1;32m    276\u001B[0m     filename \u001B[39m=\u001B[39m orig_filename[:\u001B[39m-\u001B[39m\u001B[39m3\u001B[39m] \u001B[39m+\u001B[39m \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m_part\u001B[39m\u001B[39m{\u001B[39;00mshard_idx\u001B[39m}\u001B[39;00m\u001B[39m.pt\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    277\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m PathManager\u001B[39m.\u001B[39mexists(filename):\n\u001B[0;32m--> 278\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mIOError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mModel file not found: \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m\"\u001B[39m\u001B[39m.\u001B[39mformat(filename))\n\u001B[1;32m    279\u001B[0m state \u001B[39m=\u001B[39m load_checkpoint_to_cpu(filename, arg_overrides)\n\u001B[1;32m    280\u001B[0m \u001B[39mif\u001B[39;00m shard_idx \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n",
      "\u001B[0;31mOSError\u001B[0m: Model file not found: ../PhoBERT_base_fairseq/../model.pt"
     ]
    }
   ],
   "source": [
    "phobert = RobertaModel.from_pretrained('../PhoBERT_base_fairseq', checkpoint_file='../model.pt')\n",
    "args = BPE()\n",
    "phobert.bpe = fastBPE(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_word(word, num_similar=1):\n",
    "    \"\"\"Find similar word given a word\n",
    "\n",
    "    Args:\n",
    "        word (str): Word need to find synonym\n",
    "        num_similar (int, optional): Number of generated. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of similar words\n",
    "    \"\"\"\n",
    "    ls_similar_word = []\n",
    "    \n",
    "    if word not in fasttext_data:\n",
    "        return []\n",
    "    \n",
    "    ref_v = fasttext_data[word]\n",
    "    \n",
    "    top_num = 20\n",
    "    top_w = ['']*top_num\n",
    "    top_s = [-1]*top_num\n",
    "\n",
    "    for k in fasttext_data.keys():\n",
    "        if word == k:\n",
    "            continue\n",
    "        score =  np.dot(ref_v, fasttext_data[k])\n",
    "        if score < np.min(top_s):\n",
    "            continue\n",
    "        \n",
    "        for i in range(top_num):\n",
    "            if score >= top_s[i]:\n",
    "                top_w[i+1:] = top_w[i:top_num-1]\n",
    "                top_w[i] = k\n",
    "                top_s[i+1:] = top_s[i:top_num-1]\n",
    "                top_s[i] = score\n",
    "                break\n",
    "        \n",
    "    count = 0\n",
    "    for i in range(top_num):\n",
    "        if count >= num_similar:\n",
    "            return ls_similar_word\n",
    "        \n",
    "        if top_w[i].lower() == word:\n",
    "            continue\n",
    "        \n",
    "        ls_similar_word.append(top_w[i].lower())\n",
    "        count += 1\n",
    "        \n",
    "    return ls_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi 0.8070541195329557\n",
      "anh 0.6859056984666219\n",
      "muốn 0.6335938538320789\n",
      "nghĩ 0.6285581462391725\n",
      "mình 0.6075433200828794\n",
      "thì 0.6056370953612328\n",
      "Chắc 0.5955072270317948\n",
      "cậu 0.5888613733823663\n",
      "cũng 0.5821518868888097\n",
      "chị 0.5752282965892783\n",
      "biết 0.5721677601666934\n",
      "nhưng 0.5715116778715326\n",
      "nên 0.5706017748580544\n",
      "thấy 0.567863576400363\n",
      "rồi 0.56736808957184\n",
      "Nhưng 0.5669557116514035\n",
      "ông 0.5668420945218877\n",
      "nếu 0.563732383604816\n",
      "bạn 0.5599824338728905\n",
      "khi 0.5580050167609801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anh', 'muốn', 'nghĩ', 'mình', 'thì']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(\"tôi đi học\", num_similar=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "TOKENIZER_REGEX = re.compile(r'(\\W)')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = TOKENIZER_REGEX.split(text)\n",
    "    return [t for t in tokens if len(t.strip()) > 0]\n",
    "\n",
    "class RandomSynonymInsert:\n",
    "    def __init__(self, p_aug=0.1, min_aug=1, max_aug=10):\n",
    "        self.p_aug = p_aug\n",
    "        self.min_aug = min_aug\n",
    "        self.max_aug = max_aug \n",
    "\n",
    "    def _transform(self, token, n_tokens):\n",
    "        print(f\"Processing token: {token}\")\n",
    "        synonyms = find_similar_word(token, num_similar=5)\n",
    "        print(synonyms)\n",
    "        chosen_word = random.choice(synonyms)\n",
    "        chosen_idx = random.choice(range(n_tokens))\n",
    "        \n",
    "        return chosen_word, chosen_idx\n",
    "\n",
    "    def augment(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        \n",
    "        if len(tokens) < 3:\n",
    "            return text\n",
    "    \n",
    "        augmented_tokens = []\n",
    "        new_sent = tokens.copy()\n",
    "        \n",
    "        num_aug = 0\n",
    "        for token in tokens:\n",
    "            if num_aug == self.max_aug:\n",
    "                break\n",
    "            \n",
    "            if token in string.punctuation or random.uniform(0, 1) > self.p_aug:\n",
    "                continue\n",
    "\n",
    "            chosen_word, chosen_idx = self._transform(token, len(new_sent))\n",
    "            \n",
    "            new_sent.insert(chosen_idx, chosen_word)\n",
    "            augmented_tokens.append(token)\n",
    "            num_aug += 1\n",
    "        \n",
    "        if num_aug < self.min_aug:\n",
    "            for _ in range(num_aug, self.min_aug):\n",
    "                token = \".\"\n",
    "                while token not in string.punctuation and token not in augmented_tokens:\n",
    "                    token = random.choice(tokens)\n",
    "                \n",
    "                chosen_word, chosen_idx = self._transform(token, len(new_sent))\n",
    "                new_sent.insert(chosen_idx, chosen_word)\n",
    "                \n",
    "                tokens.insert(chosen_idx, chosen_word)     \n",
    "    \n",
    "        return \" \".join(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token: vô\n",
      "['hư', 'hễ', 'xô', 'nó', 'bả']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mang nó vô đi'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_syn = RandomSynonymInsert(p_aug=0.1, min_aug=1, max_aug=2)\n",
    "rand_syn.augment(\"mang vô đi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def mask_exclude_tokens(text, exclude):\n",
    "    exclude_map = {}\n",
    "    count = 0\n",
    "    mask = \"MASK\"\n",
    "\n",
    "    for token in exclude:\n",
    "        tmp_mask = mask + str(count)\n",
    "        exclude_map[tmp_mask] = token\n",
    "        text = re.sub(token, tmp_mask, text, flags=re.IGNORECASE)\n",
    "        count += 1\n",
    "    return text, exclude_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'Lợi_ích lớn nhất của thuỷ_điện là hạn_chế được giá_thành nhiên_liệu . Các MASK0 thuỷ_điện không phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu . Các MASK0 thuỷ_điện cũng có tuổi_thọ lớn hơn các MASK0 nhiệt_điện , một_số MASK0 thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . MASK1 Chi_phí nhân_công cũng thấp bởi_vì MASK3 các MASK0 này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường </ANS> ng phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu MASK2 . Các MASK0 thuỷ_điện cũng có tuổi_thọ lớn hơn các MASK0 nhiệt_điện , một_số MASK0 thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . Chi_phí nhân_công cũng thấp bởi_vì các MASK0 này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường .'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, exclude_map = mask_exclude_tokens('Lợi_ích lớn nhất của thuỷ_điện là hạn_chế được giá_thành nhiên_liệu . Các nhà_máy thuỷ_điện không phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu . Các nhà_máy thuỷ_điện cũng có tuổi_thọ lớn hơn các nhà_máy nhiệt_điện , một_số nhà_máy thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . <CLUE> Chi_phí nhân_công cũng thấp bởi_vì <ANS> các nhà_máy này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường </ANS> ng phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu </CLUE> . Các nhà_máy thuỷ_điện cũng có tuổi_thọ lớn hơn các nhà_máy nhiệt_điện , một_số nhà_máy thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . Chi_phí nhân_công cũng thấp bởi_vì các nhà_máy này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường .', [\"nhà_máy\", \"<CLUE>\", \"</CLUE>\", \"<ANS>\"])\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def reconstruct(text, exclude_map):\n",
    "    for key, value in exclude_map.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'Lợi_ích lớn nhất của thuỷ_điện là hạn_chế được giá_thành nhiên_liệu . Các nhà_máy thuỷ_điện không phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu . Các nhà_máy thuỷ_điện cũng có tuổi_thọ lớn hơn các nhà_máy nhiệt_điện , một_số nhà_máy thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . <CLUE> Chi_phí nhân_công cũng thấp bởi_vì <ANS> các nhà_máy này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường </ANS> ng phải chịu cảnh tăng_giá của nhiên_liệu hoá_thạch như dầu_mỏ , khí_thiên_nhiên hay than_đá , và không cần phải nhập nhiên_liệu </CLUE> . Các nhà_máy thuỷ_điện cũng có tuổi_thọ lớn hơn các nhà_máy nhiệt_điện , một_số nhà_máy thuỷ_điện đang hoạt_động hiện_nay đã được xây_dựng từ 50 đến 100 năm trước . Chi_phí nhân_công cũng thấp bởi_vì các nhà_máy này được tự_động_hoá cao và có ít người làm_việc tại_chỗ khi vận_hành thông_thường .'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct(text, exclude_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "text = \"hôm nay tôi đi học. hôm mai tôi đi làm\"\n",
    "text.replace(\"hôm\", \"mask_1\")\n",
    "time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('augmentation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3412f85487f9bd59d8f08c56a9e81f1227eaabe6ccec5e900a8c733b6c32a271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from underthesea import word_tokenize\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "from fairseq.data.encoders.fastbpe import fastBPE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_vectors(fname, num_words=10000):\n",
    "    \"\"\"Load fasttext vector\n",
    "\n",
    "    Args:\n",
    "        fname (str): file path\n",
    "        num_words (int, optional): Number of words to be loaded. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary of word\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    fin.readline().split()\n",
    "    data = {}\n",
    "    for i, line in enumerate(tqdm(fin)):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array([float(val) for val in tokens[1:]])\n",
    "        data[tokens[0]] /= np.linalg.norm(data[tokens[0]])\n",
    "        if i > num_words:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [00:01, 6386.85it/s]\n"
     ]
    }
   ],
   "source": [
    "FASTTEXT_PATH = \"../augment_model/cc.vi.300.vec\"\n",
    "fasttext_data = _load_vectors(FASTTEXT_PATH)\n",
    "\n",
    "class BPE():\n",
    "    bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 17:04:38 | INFO | fairseq.file_utils | loading archive file ../PhoBERT_base_fairseq\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Model file not found: ../PhoBERT_base_fairseq/../model.pt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m phobert \u001b[39m=\u001b[39m RobertaModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39m../PhoBERT_base_fairseq\u001b[39;49m\u001b[39m'\u001b[39;49m, checkpoint_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../model.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m args \u001b[39m=\u001b[39m BPE()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/AI_Member/hieunt/augmentation/notebooks/test.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m phobert\u001b[39m.\u001b[39mbpe \u001b[39m=\u001b[39m fastBPE(args)\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/models/roberta/model.py:244\u001b[0m, in \u001b[0;36mRobertaModel.from_pretrained\u001b[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, bpe, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[1;32m    235\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    241\u001b[0m ):\n\u001b[1;32m    242\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m \u001b[39mimport\u001b[39;00m hub_utils\n\u001b[0;32m--> 244\u001b[0m     x \u001b[39m=\u001b[39m hub_utils\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    245\u001b[0m         model_name_or_path,\n\u001b[1;32m    246\u001b[0m         checkpoint_file,\n\u001b[1;32m    247\u001b[0m         data_name_or_path,\n\u001b[1;32m    248\u001b[0m         archive_map\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mhub_models(),\n\u001b[1;32m    249\u001b[0m         bpe\u001b[39m=\u001b[39;49mbpe,\n\u001b[1;32m    250\u001b[0m         load_checkpoint_heads\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    251\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m RobertaHubInterface(x[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m], x[\u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m], x[\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/hub_utils.py:70\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser_dir\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m     68\u001b[0m     utils\u001b[39m.\u001b[39mimport_user_module(argparse\u001b[39m.\u001b[39mNamespace(user_dir\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39muser_dir\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m---> 70\u001b[0m models, args, task \u001b[39m=\u001b[39m checkpoint_utils\u001b[39m.\u001b[39;49mload_model_ensemble_and_task(\n\u001b[1;32m     71\u001b[0m     [os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, cpt) \u001b[39mfor\u001b[39;49;00m cpt \u001b[39min\u001b[39;49;00m checkpoint_file\u001b[39m.\u001b[39;49msplit(os\u001b[39m.\u001b[39;49mpathsep)],\n\u001b[1;32m     72\u001b[0m     arg_overrides\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m: args,\n\u001b[1;32m     77\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m: task,\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m: models,\n\u001b[1;32m     79\u001b[0m }\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/augmentation/lib/python3.9/site-packages/fairseq/checkpoint_utils.py:278\u001b[0m, in \u001b[0;36mload_model_ensemble_and_task\u001b[0;34m(filenames, arg_overrides, task, strict, suffix, num_shards)\u001b[0m\n\u001b[1;32m    276\u001b[0m     filename \u001b[39m=\u001b[39m orig_filename[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_part\u001b[39m\u001b[39m{\u001b[39;00mshard_idx\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m PathManager\u001b[39m.\u001b[39mexists(filename):\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mModel file not found: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(filename))\n\u001b[1;32m    279\u001b[0m state \u001b[39m=\u001b[39m load_checkpoint_to_cpu(filename, arg_overrides)\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m shard_idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Model file not found: ../PhoBERT_base_fairseq/../model.pt"
     ]
    }
   ],
   "source": [
    "phobert = RobertaModel.from_pretrained('../PhoBERT_base_fairseq', checkpoint_file='../model.pt')\n",
    "args = BPE()\n",
    "phobert.bpe = fastBPE(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_word(word, num_similar=1):\n",
    "    \"\"\"Find similar word given a word\n",
    "\n",
    "    Args:\n",
    "        word (str): Word need to find synonym\n",
    "        num_similar (int, optional): Number of generated. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of similar words\n",
    "    \"\"\"\n",
    "    ls_similar_word = []\n",
    "    \n",
    "    if word not in fasttext_data:\n",
    "        return []\n",
    "    \n",
    "    ref_v = fasttext_data[word]\n",
    "    \n",
    "    top_num = 20\n",
    "    top_w = ['']*top_num\n",
    "    top_s = [-1]*top_num\n",
    "\n",
    "    for k in fasttext_data.keys():\n",
    "        if word == k:\n",
    "            continue\n",
    "        score =  np.dot(ref_v, fasttext_data[k])\n",
    "        if score < np.min(top_s):\n",
    "            continue\n",
    "        \n",
    "        for i in range(top_num):\n",
    "            if score >= top_s[i]:\n",
    "                top_w[i+1:] = top_w[i:top_num-1]\n",
    "                top_w[i] = k\n",
    "                top_s[i+1:] = top_s[i:top_num-1]\n",
    "                top_s[i] = score\n",
    "                break\n",
    "        \n",
    "    count = 0\n",
    "    for i in range(top_num):\n",
    "        if count >= num_similar:\n",
    "            return ls_similar_word\n",
    "        \n",
    "        if top_w[i].lower() == word:\n",
    "            continue\n",
    "        \n",
    "        ls_similar_word.append(top_w[i].lower())\n",
    "        count += 1\n",
    "        \n",
    "    return ls_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi 0.8070541195329557\n",
      "anh 0.6859056984666219\n",
      "muốn 0.6335938538320789\n",
      "nghĩ 0.6285581462391725\n",
      "mình 0.6075433200828794\n",
      "thì 0.6056370953612328\n",
      "Chắc 0.5955072270317948\n",
      "cậu 0.5888613733823663\n",
      "cũng 0.5821518868888097\n",
      "chị 0.5752282965892783\n",
      "biết 0.5721677601666934\n",
      "nhưng 0.5715116778715326\n",
      "nên 0.5706017748580544\n",
      "thấy 0.567863576400363\n",
      "rồi 0.56736808957184\n",
      "Nhưng 0.5669557116514035\n",
      "ông 0.5668420945218877\n",
      "nếu 0.563732383604816\n",
      "bạn 0.5599824338728905\n",
      "khi 0.5580050167609801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anh', 'muốn', 'nghĩ', 'mình', 'thì']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_word(\"tôi đi học\", num_similar=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "TOKENIZER_REGEX = re.compile(r'(\\W)')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = TOKENIZER_REGEX.split(text)\n",
    "    return [t for t in tokens if len(t.strip()) > 0]\n",
    "\n",
    "class RandomSynonymInsert:\n",
    "    def __init__(self, p_aug=0.1, min_aug=1, max_aug=10):\n",
    "        self.p_aug = p_aug\n",
    "        self.min_aug = min_aug\n",
    "        self.max_aug = max_aug \n",
    "\n",
    "    def _transform(self, token, n_tokens):\n",
    "        print(f\"Processing token: {token}\")\n",
    "        synonyms = find_similar_word(token, num_similar=5)\n",
    "        print(synonyms)\n",
    "        chosen_word = random.choice(synonyms)\n",
    "        chosen_idx = random.choice(range(n_tokens))\n",
    "        \n",
    "        return chosen_word, chosen_idx\n",
    "\n",
    "    def augment(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        \n",
    "        if len(tokens) < 3:\n",
    "            return text\n",
    "    \n",
    "        augmented_tokens = []\n",
    "        new_sent = tokens.copy()\n",
    "        \n",
    "        num_aug = 0\n",
    "        for token in tokens:\n",
    "            if num_aug == self.max_aug:\n",
    "                break\n",
    "            \n",
    "            if token in string.punctuation or random.uniform(0, 1) > self.p_aug:\n",
    "                continue\n",
    "\n",
    "            chosen_word, chosen_idx = self._transform(token, len(new_sent))\n",
    "            \n",
    "            new_sent.insert(chosen_idx, chosen_word)\n",
    "            augmented_tokens.append(token)\n",
    "            num_aug += 1\n",
    "        \n",
    "        if num_aug < self.min_aug:\n",
    "            for _ in range(num_aug, self.min_aug):\n",
    "                token = \".\"\n",
    "                while token not in string.punctuation and token not in augmented_tokens:\n",
    "                    token = random.choice(tokens)\n",
    "                \n",
    "                chosen_word, chosen_idx = self._transform(token, len(new_sent))\n",
    "                new_sent.insert(chosen_idx, chosen_word)\n",
    "                \n",
    "                tokens.insert(chosen_idx, chosen_word)     \n",
    "    \n",
    "        return \" \".join(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token: vô\n",
      "['hư', 'hễ', 'xô', 'nó', 'bả']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mang nó vô đi'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_syn = RandomSynonymInsert(p_aug=0.1, min_aug=1, max_aug=2)\n",
    "rand_syn.augment(\"mang vô đi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('augmentation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3412f85487f9bd59d8f08c56a9e81f1227eaabe6ccec5e900a8c733b6c32a271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
